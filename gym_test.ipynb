{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import gym.spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make(\"FrozenLake-v1\", is_slippery=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Discrete(4), 16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.action_space, environment.observation_space.n, \n",
    "#environment.env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(OneHotWrapper,self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype = np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        r = np.copy(self.observation_space.low)\n",
    "        r[observation] = 1.0\n",
    "        return r\n",
    "\n",
    "environment = OneHotWrapper(environment)\n",
    "\n",
    "# class OneHotWrapper(gym.ObservationWrapper):\n",
    "# def __init__(self, env):\n",
    "#    super(OneHotWrapper, self).__init__(env)\n",
    "#    self.observation_space = gym.spaces.Box(0.0, 1.0, \n",
    "#               (env.observation_space.n, ), dtype=np.float32)\n",
    "# def observation(self, observation):\n",
    "#     r = np.copy(self.observation_space.low)\n",
    "#     r[observation] = 1.0\n",
    "#     return r\n",
    "\n",
    "# env = OneHotWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 4\n"
     ]
    }
   ],
   "source": [
    "obs_size = environment.observation_space.shape[0]\n",
    "n_actions = environment.action_space.n\n",
    "\n",
    "print(obs_size, n_actions)\n",
    "\n",
    "HIDDEN_SIZE = 32\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(obs_size, HIDDEN_SIZE),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(HIDDEN_SIZE, n_actions)\n",
    ")\n",
    "\n",
    "# obs_size = env.observation_space.shape[0]\n",
    "# n_actions = env.action_space.n\n",
    "# HIDDEN_SIZE = 32\n",
    "# net= nn.Sequential(\n",
    "#      nn.Linear(obs_size, HIDDEN_SIZE),\n",
    "#      nn.Sigmoid(),\n",
    "#      nn.Linear(HIDDEN_SIZE, n_actions)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get an action\n",
    "sm = nn.Softmax(dim=1)\n",
    "\n",
    "def select_action(state):\n",
    "    state_tensor = torch.FloatTensor([state])\n",
    "    act_probs_tensor = sm( net(state_tensor) )\n",
    "    act_probs = act_probs_tensor.data.numpy()\n",
    "    act_probs = act_probs[0]\n",
    "    action = np.random.choice(len(act_probs), p = act_probs )\n",
    "    return action\n",
    "\n",
    "initial_state = environment.reset()\n",
    "#print(initial_state)\n",
    "select_action(initial_state)\n",
    "# sm = nn.Softmax(dim=1)\n",
    "#    def select_action(state):\n",
    "# 1:      state_t = torch.FloatTensor([state])\n",
    "# 2:      act_probs_t = sm(net(state_t))\n",
    "# 3:      act_probs = act_probs_t.data.numpy()[0]\n",
    "# 4:      action = np.random.choice(len(act_probs), p=act_probs)\n",
    "#         return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy = [1.0, 2.0, 1.0, 4.5]\n",
    "# dummy_float_tensor = torch.FloatTensor(dummy)\n",
    "\n",
    "# dummy_float_tensor.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "GAMMA = 0.9\n",
    "\n",
    "PERCENTILE = 30\n",
    "REWARD_GOAL = 0.8\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward bound 0.0\n",
      "0: loss=1.427, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "1: loss=1.380, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "2: loss=1.395, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "3: loss=1.365, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "Reward bound 0.0\n",
      "4: loss=1.402, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "5: loss=1.344, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "6: loss=1.387, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "7: loss=1.373, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "8: loss=1.396, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "9: loss=1.348, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "10: loss=1.350, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "Reward bound 0.0\n",
      "11: loss=1.352, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "12: loss=1.383, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "13: loss=1.342, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "14: loss=1.395, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "15: loss=1.338, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "16: loss=1.387, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "17: loss=1.358, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "18: loss=1.347, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "19: loss=1.341, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "20: loss=1.331, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "21: loss=1.322, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "22: loss=1.363, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "23: loss=1.320, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "24: loss=1.340, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "25: loss=1.320, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "26: loss=1.347, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "27: loss=1.371, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "28: loss=1.339, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "29: loss=1.355, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "Reward bound 0.0\n",
      "30: loss=1.350, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "31: loss=1.354, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "32: loss=1.328, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "33: loss=1.339, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "34: loss=1.269, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "35: loss=1.360, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "36: loss=1.311, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "37: loss=1.390, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "38: loss=1.334, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "39: loss=1.242, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "40: loss=1.261, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "41: loss=1.312, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "42: loss=1.307, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "Reward bound 0.0\n",
      "43: loss=1.293, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "44: loss=1.307, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "45: loss=1.309, reward_mean=0.100\n",
      "Reward bound 0.0\n",
      "46: loss=1.222, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "47: loss=1.320, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "48: loss=1.383, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "49: loss=1.246, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "50: loss=1.284, reward_mean=0.050\n",
      "Reward bound 0.0\n",
      "51: loss=1.210, reward_mean=0.010\n",
      "Reward bound 0.0\n",
      "52: loss=1.289, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "53: loss=1.296, reward_mean=0.060\n",
      "Reward bound 0.0\n",
      "54: loss=1.200, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "55: loss=1.266, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "56: loss=1.294, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "57: loss=1.227, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "58: loss=1.241, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "59: loss=1.325, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "60: loss=1.260, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "61: loss=1.295, reward_mean=0.060\n",
      "Reward bound 0.0\n",
      "62: loss=1.258, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "63: loss=1.282, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "64: loss=1.317, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "65: loss=1.267, reward_mean=0.060\n",
      "Reward bound 0.0\n",
      "66: loss=1.295, reward_mean=0.060\n",
      "Reward bound 0.0\n",
      "67: loss=1.288, reward_mean=0.080\n",
      "Reward bound 0.0\n",
      "68: loss=1.331, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "69: loss=1.195, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "70: loss=1.273, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "71: loss=1.287, reward_mean=0.060\n",
      "Reward bound 0.0\n",
      "72: loss=1.238, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "73: loss=1.246, reward_mean=0.050\n",
      "Reward bound 0.0\n",
      "74: loss=1.236, reward_mean=0.070\n",
      "Reward bound 0.0\n",
      "75: loss=1.282, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "76: loss=1.269, reward_mean=0.070\n",
      "Reward bound 0.0\n",
      "77: loss=1.293, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "78: loss=1.264, reward_mean=0.050\n",
      "Reward bound 0.0\n",
      "79: loss=1.268, reward_mean=0.060\n",
      "Reward bound 0.0\n",
      "80: loss=1.215, reward_mean=0.050\n",
      "Reward bound 0.0\n",
      "81: loss=1.240, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "82: loss=1.200, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "Reward bound 0.0\n",
      "83: loss=1.314, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "84: loss=1.232, reward_mean=0.070\n",
      "Reward bound 0.0\n",
      "85: loss=1.260, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "86: loss=1.196, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "87: loss=1.094, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "88: loss=1.305, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "89: loss=1.179, reward_mean=0.080\n",
      "Reward bound 0.0\n",
      "90: loss=1.176, reward_mean=0.050\n",
      "Reward bound 0.0\n",
      "91: loss=1.246, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "92: loss=1.150, reward_mean=0.020\n",
      "Reward bound 0.0\n",
      "93: loss=1.190, reward_mean=0.090\n",
      "Reward bound 0.0\n",
      "94: loss=1.190, reward_mean=0.110\n",
      "Reward bound 0.0\n",
      "95: loss=1.191, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "96: loss=1.210, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "97: loss=1.260, reward_mean=0.090\n",
      "Reward bound 0.0\n",
      "98: loss=1.184, reward_mean=0.060\n",
      "Reward bound 0.0\n",
      "99: loss=1.211, reward_mean=0.070\n",
      "Reward bound 0.0\n",
      "100: loss=1.206, reward_mean=0.050\n",
      "Reward bound 0.0\n",
      "101: loss=1.125, reward_mean=0.090\n",
      "Reward bound 0.0\n",
      "102: loss=1.284, reward_mean=0.030\n",
      "Reward bound 0.0\n",
      "103: loss=1.211, reward_mean=0.080\n",
      "Reward bound 0.0\n",
      "104: loss=1.210, reward_mean=0.060\n",
      "Reward bound 0.0\n",
      "105: loss=1.161, reward_mean=0.090\n",
      "Reward bound 0.0\n",
      "106: loss=1.225, reward_mean=0.060\n",
      "Reward bound 0.0\n",
      "107: loss=1.155, reward_mean=0.080\n",
      "Reward bound 0.0\n",
      "108: loss=1.119, reward_mean=0.050\n",
      "Reward bound 0.0\n",
      "109: loss=1.101, reward_mean=0.050\n",
      "Reward bound 0.0\n",
      "110: loss=1.172, reward_mean=0.080\n",
      "Reward bound 0.0\n",
      "111: loss=1.160, reward_mean=0.060\n",
      "Reward bound 0.0\n",
      "112: loss=1.224, reward_mean=0.140\n",
      "Reward bound 0.0\n",
      "113: loss=1.282, reward_mean=0.050\n",
      "Reward bound 0.0\n",
      "114: loss=1.244, reward_mean=0.060\n",
      "Reward bound 0.0\n",
      "115: loss=1.236, reward_mean=0.050\n",
      "Reward bound 0.0\n",
      "116: loss=1.174, reward_mean=0.090\n",
      "Reward bound 0.0\n",
      "117: loss=1.185, reward_mean=0.090\n",
      "Reward bound 0.0\n",
      "118: loss=1.216, reward_mean=0.110\n",
      "Reward bound 0.0\n",
      "119: loss=1.217, reward_mean=0.100\n",
      "Reward bound 0.0\n",
      "120: loss=1.136, reward_mean=0.060\n",
      "Reward bound 0.0\n",
      "121: loss=1.254, reward_mean=0.040\n",
      "Reward bound 0.0\n",
      "122: loss=1.169, reward_mean=0.100\n",
      "Reward bound 0.0\n",
      "123: loss=1.133, reward_mean=0.120\n",
      "Reward bound 0.0\n",
      "124: loss=1.148, reward_mean=0.130\n",
      "Reward bound 0.0\n",
      "125: loss=1.187, reward_mean=0.050\n",
      "Reward bound 0.0\n",
      "126: loss=1.179, reward_mean=0.090\n",
      "Reward bound 0.0\n",
      "127: loss=1.153, reward_mean=0.110\n",
      "Reward bound 0.0\n",
      "128: loss=1.164, reward_mean=0.120\n",
      "Reward bound 0.0\n",
      "129: loss=1.100, reward_mean=0.090\n",
      "Reward bound 0.0\n",
      "130: loss=1.156, reward_mean=0.160\n",
      "Reward bound 0.0\n",
      "131: loss=1.132, reward_mean=0.230\n",
      "Reward bound 0.0\n",
      "132: loss=1.083, reward_mean=0.120\n",
      "Reward bound 0.0\n",
      "133: loss=1.085, reward_mean=0.110\n",
      "Reward bound 0.0\n",
      "134: loss=1.126, reward_mean=0.110\n",
      "Reward bound 0.0\n",
      "135: loss=1.158, reward_mean=0.110\n",
      "Reward bound 0.0\n",
      "136: loss=1.066, reward_mean=0.180\n",
      "Reward bound 0.0\n",
      "137: loss=1.138, reward_mean=0.120\n",
      "Reward bound 0.0\n",
      "138: loss=1.114, reward_mean=0.130\n",
      "Reward bound 0.0\n",
      "139: loss=1.158, reward_mean=0.130\n",
      "Reward bound 0.0\n",
      "140: loss=1.143, reward_mean=0.100\n",
      "Reward bound 0.0\n",
      "141: loss=1.174, reward_mean=0.110\n",
      "Reward bound 0.0\n",
      "142: loss=1.133, reward_mean=0.060\n",
      "Reward bound 0.0\n",
      "143: loss=1.066, reward_mean=0.130\n",
      "Reward bound 0.0\n",
      "144: loss=1.130, reward_mean=0.070\n",
      "Reward bound 0.0\n",
      "145: loss=1.095, reward_mean=0.130\n",
      "Reward bound 0.0\n",
      "146: loss=1.146, reward_mean=0.090\n",
      "Reward bound 0.0\n",
      "147: loss=1.137, reward_mean=0.090\n",
      "Reward bound 0.0\n",
      "148: loss=1.104, reward_mean=0.160\n",
      "Reward bound 0.0\n",
      "149: loss=1.063, reward_mean=0.160\n",
      "Reward bound 0.0\n",
      "150: loss=1.078, reward_mean=0.180\n",
      "Reward bound 0.0\n",
      "151: loss=1.080, reward_mean=0.140\n",
      "Reward bound 0.0\n",
      "152: loss=1.081, reward_mean=0.120\n",
      "Reward bound 0.0\n",
      "153: loss=1.116, reward_mean=0.190\n",
      "Reward bound 0.0\n",
      "154: loss=1.119, reward_mean=0.090\n",
      "Reward bound 0.0\n",
      "155: loss=1.048, reward_mean=0.160\n",
      "Reward bound 0.0\n",
      "156: loss=1.073, reward_mean=0.140\n",
      "Reward bound 0.0\n",
      "157: loss=1.078, reward_mean=0.170\n",
      "Reward bound 0.0\n",
      "158: loss=1.039, reward_mean=0.150\n",
      "Reward bound 0.0\n",
      "159: loss=1.095, reward_mean=0.160\n",
      "Reward bound 0.0\n",
      "160: loss=1.120, reward_mean=0.200\n",
      "Reward bound 0.0\n",
      "161: loss=1.039, reward_mean=0.210\n",
      "Reward bound 0.0\n",
      "162: loss=1.006, reward_mean=0.170\n",
      "Reward bound 0.0\n",
      "163: loss=1.072, reward_mean=0.140\n",
      "Reward bound 0.0\n",
      "164: loss=1.088, reward_mean=0.140\n",
      "Reward bound 0.0\n",
      "165: loss=1.089, reward_mean=0.170\n",
      "Reward bound 0.0\n",
      "166: loss=1.052, reward_mean=0.220\n",
      "Reward bound 0.0\n",
      "167: loss=1.055, reward_mean=0.170\n",
      "Reward bound 0.0\n",
      "168: loss=1.033, reward_mean=0.210\n",
      "Reward bound 0.0\n",
      "169: loss=1.061, reward_mean=0.120\n",
      "Reward bound 0.0\n",
      "170: loss=1.055, reward_mean=0.200\n",
      "Reward bound 0.0\n",
      "171: loss=1.021, reward_mean=0.190\n",
      "Reward bound 0.0\n",
      "172: loss=1.055, reward_mean=0.130\n",
      "Reward bound 0.0\n",
      "173: loss=1.066, reward_mean=0.170\n",
      "Reward bound 0.0\n",
      "174: loss=0.991, reward_mean=0.190\n",
      "Reward bound 0.0\n",
      "175: loss=0.985, reward_mean=0.170\n",
      "Reward bound 0.0\n",
      "176: loss=0.966, reward_mean=0.170\n",
      "Reward bound 0.0\n",
      "177: loss=0.969, reward_mean=0.170\n",
      "Reward bound 0.0\n",
      "178: loss=0.974, reward_mean=0.250\n",
      "Reward bound 0.0\n",
      "179: loss=1.049, reward_mean=0.200\n",
      "Reward bound 0.0\n",
      "180: loss=1.043, reward_mean=0.250\n",
      "Reward bound 0.0\n",
      "181: loss=1.013, reward_mean=0.270\n",
      "Reward bound 0.0\n",
      "182: loss=1.060, reward_mean=0.180\n",
      "Reward bound 0.0\n",
      "183: loss=0.935, reward_mean=0.200\n",
      "Reward bound 0.0\n",
      "184: loss=0.994, reward_mean=0.260\n",
      "Reward bound 0.0\n",
      "185: loss=0.945, reward_mean=0.220\n",
      "Reward bound 0.0\n",
      "186: loss=0.956, reward_mean=0.230\n",
      "Reward bound 0.0\n",
      "187: loss=1.034, reward_mean=0.190\n",
      "Reward bound 0.0\n",
      "188: loss=0.941, reward_mean=0.280\n",
      "Reward bound 0.0\n",
      "189: loss=0.933, reward_mean=0.240\n",
      "Reward bound 0.0\n",
      "190: loss=0.937, reward_mean=0.200\n",
      "Reward bound 0.0\n",
      "191: loss=0.971, reward_mean=0.210\n",
      "Reward bound 0.0\n",
      "192: loss=1.008, reward_mean=0.250\n",
      "Reward bound 0.0\n",
      "193: loss=0.997, reward_mean=0.250\n",
      "Reward bound 0.0\n",
      "194: loss=1.015, reward_mean=0.260\n",
      "Reward bound 0.0\n",
      "195: loss=0.947, reward_mean=0.230\n",
      "Reward bound 0.0\n",
      "196: loss=0.977, reward_mean=0.230\n",
      "Reward bound 0.0\n",
      "197: loss=1.018, reward_mean=0.320\n",
      "Reward bound 0.0\n",
      "198: loss=1.006, reward_mean=0.250\n",
      "Reward bound 0.0\n",
      "199: loss=0.956, reward_mean=0.320\n",
      "Reward bound 0.0\n",
      "200: loss=0.945, reward_mean=0.220\n",
      "Reward bound 0.0\n",
      "201: loss=0.927, reward_mean=0.250\n",
      "Reward bound 0.0\n",
      "202: loss=0.956, reward_mean=0.240\n",
      "Reward bound 0.0\n",
      "203: loss=0.962, reward_mean=0.270\n",
      "Reward bound 0.0\n",
      "204: loss=0.972, reward_mean=0.210\n",
      "Reward bound 0.0\n",
      "205: loss=0.952, reward_mean=0.230\n",
      "Reward bound 0.0\n",
      "206: loss=0.944, reward_mean=0.300\n",
      "Reward bound 0.0\n",
      "207: loss=0.965, reward_mean=0.280\n",
      "Reward bound 0.0\n",
      "208: loss=0.982, reward_mean=0.300\n",
      "Reward bound 0.0\n",
      "209: loss=0.923, reward_mean=0.290\n",
      "Reward bound 0.0\n",
      "210: loss=0.974, reward_mean=0.280\n",
      "Reward bound 0.0\n",
      "211: loss=0.909, reward_mean=0.320\n",
      "Reward bound 0.0\n",
      "212: loss=0.885, reward_mean=0.250\n",
      "Reward bound 0.0\n",
      "213: loss=0.959, reward_mean=0.300\n",
      "Reward bound 0.0\n",
      "214: loss=0.900, reward_mean=0.320\n",
      "Reward bound 0.0\n",
      "215: loss=0.911, reward_mean=0.290\n",
      "Reward bound 0.0\n",
      "216: loss=0.909, reward_mean=0.320\n",
      "Reward bound 0.0\n",
      "217: loss=0.925, reward_mean=0.380\n",
      "Reward bound 0.0\n",
      "218: loss=0.912, reward_mean=0.270\n",
      "Reward bound 0.0\n",
      "219: loss=0.927, reward_mean=0.330\n",
      "Reward bound 0.0\n",
      "220: loss=0.882, reward_mean=0.360\n",
      "Reward bound 0.0\n",
      "221: loss=0.920, reward_mean=0.320\n",
      "Reward bound 0.0\n",
      "222: loss=0.920, reward_mean=0.270\n",
      "Reward bound 0.0\n",
      "223: loss=0.892, reward_mean=0.310\n",
      "Reward bound 0.0\n",
      "224: loss=0.928, reward_mean=0.350\n",
      "Reward bound 0.0\n",
      "225: loss=0.920, reward_mean=0.440\n",
      "Reward bound 0.0\n",
      "226: loss=0.871, reward_mean=0.270\n",
      "Reward bound 0.0\n",
      "227: loss=0.971, reward_mean=0.380\n",
      "Reward bound 0.0\n",
      "228: loss=0.881, reward_mean=0.360\n",
      "Reward bound 0.0\n",
      "229: loss=0.882, reward_mean=0.350\n",
      "Reward bound 0.0\n",
      "230: loss=0.852, reward_mean=0.410\n",
      "Reward bound 0.0\n",
      "231: loss=0.905, reward_mean=0.330\n",
      "Reward bound 0.0\n",
      "232: loss=0.885, reward_mean=0.380\n",
      "Reward bound 0.0\n",
      "233: loss=0.842, reward_mean=0.380\n",
      "Reward bound 0.0\n",
      "234: loss=0.929, reward_mean=0.420\n",
      "Reward bound 0.0\n",
      "235: loss=0.840, reward_mean=0.340\n",
      "Reward bound 0.0\n",
      "236: loss=0.944, reward_mean=0.330\n",
      "Reward bound 0.0\n",
      "237: loss=0.840, reward_mean=0.360\n",
      "Reward bound 0.0\n",
      "238: loss=0.879, reward_mean=0.370\n",
      "Reward bound 0.0\n",
      "239: loss=0.873, reward_mean=0.360\n",
      "Reward bound 0.0\n",
      "240: loss=0.860, reward_mean=0.470\n",
      "Reward bound 0.0\n",
      "241: loss=0.846, reward_mean=0.370\n",
      "Reward bound 0.0\n",
      "242: loss=0.876, reward_mean=0.420\n",
      "Reward bound 0.0\n",
      "243: loss=0.884, reward_mean=0.390\n",
      "Reward bound 0.0\n",
      "244: loss=0.817, reward_mean=0.380\n",
      "Reward bound 0.0\n",
      "245: loss=0.906, reward_mean=0.450\n",
      "Reward bound 0.0\n",
      "246: loss=0.839, reward_mean=0.350\n",
      "Reward bound 0.0\n",
      "247: loss=0.879, reward_mean=0.450\n",
      "Reward bound 0.0\n",
      "248: loss=0.880, reward_mean=0.490\n",
      "Reward bound 0.0\n",
      "249: loss=0.856, reward_mean=0.450\n",
      "Reward bound 0.0\n",
      "250: loss=0.844, reward_mean=0.390\n",
      "Reward bound 0.0\n",
      "251: loss=0.799, reward_mean=0.360\n",
      "Reward bound 0.0\n",
      "252: loss=0.821, reward_mean=0.380\n",
      "Reward bound 0.0\n",
      "253: loss=0.837, reward_mean=0.370\n",
      "Reward bound 0.0\n",
      "254: loss=0.891, reward_mean=0.480\n",
      "Reward bound 0.0\n",
      "255: loss=0.833, reward_mean=0.360\n",
      "Reward bound 0.0\n",
      "256: loss=0.839, reward_mean=0.440\n",
      "Reward bound 0.0\n",
      "257: loss=0.825, reward_mean=0.520\n",
      "Reward bound 0.0\n",
      "258: loss=0.867, reward_mean=0.570\n",
      "Reward bound 0.0\n",
      "259: loss=0.824, reward_mean=0.490\n",
      "Reward bound 0.0\n",
      "260: loss=0.858, reward_mean=0.470\n",
      "Reward bound 0.0\n",
      "261: loss=0.857, reward_mean=0.390\n",
      "Reward bound 0.0\n",
      "262: loss=0.792, reward_mean=0.430\n",
      "Reward bound 0.0\n",
      "263: loss=0.791, reward_mean=0.400\n",
      "Reward bound 0.0\n",
      "264: loss=0.854, reward_mean=0.490\n",
      "Reward bound 0.0\n",
      "265: loss=0.775, reward_mean=0.410\n",
      "Reward bound 0.0\n",
      "266: loss=0.820, reward_mean=0.450\n",
      "Reward bound 0.0\n",
      "267: loss=0.820, reward_mean=0.410\n",
      "Reward bound 0.0\n",
      "268: loss=0.815, reward_mean=0.520\n",
      "Reward bound 0.0\n",
      "269: loss=0.848, reward_mean=0.460\n",
      "Reward bound 0.0\n",
      "270: loss=0.787, reward_mean=0.530\n",
      "Reward bound 0.0\n",
      "271: loss=0.871, reward_mean=0.460\n",
      "Reward bound 0.0\n",
      "272: loss=0.858, reward_mean=0.430\n",
      "Reward bound 0.0\n",
      "273: loss=0.810, reward_mean=0.430\n",
      "Reward bound 0.0\n",
      "274: loss=0.839, reward_mean=0.500\n",
      "Reward bound 0.0\n",
      "275: loss=0.781, reward_mean=0.480\n",
      "Reward bound 0.0\n",
      "276: loss=0.832, reward_mean=0.450\n",
      "Reward bound 0.0\n",
      "277: loss=0.847, reward_mean=0.500\n",
      "Reward bound 0.0\n",
      "278: loss=0.784, reward_mean=0.520\n",
      "Reward bound 0.0\n",
      "279: loss=0.804, reward_mean=0.430\n",
      "Reward bound 0.0\n",
      "280: loss=0.792, reward_mean=0.470\n",
      "Reward bound 0.0\n",
      "281: loss=0.819, reward_mean=0.470\n",
      "Reward bound 0.0\n",
      "282: loss=0.808, reward_mean=0.490\n",
      "Reward bound 0.0\n",
      "283: loss=0.771, reward_mean=0.560\n",
      "Reward bound 0.0\n",
      "284: loss=0.789, reward_mean=0.510\n",
      "Reward bound 0.0\n",
      "285: loss=0.820, reward_mean=0.410\n",
      "Reward bound 0.0\n",
      "286: loss=0.804, reward_mean=0.440\n",
      "Reward bound 0.0\n",
      "287: loss=0.749, reward_mean=0.470\n",
      "Reward bound 0.0\n",
      "288: loss=0.810, reward_mean=0.570\n",
      "Reward bound 0.0\n",
      "289: loss=0.816, reward_mean=0.550\n",
      "Reward bound 0.0\n",
      "290: loss=0.792, reward_mean=0.520\n",
      "Reward bound 0.0\n",
      "291: loss=0.752, reward_mean=0.630\n",
      "Reward bound 0.0\n",
      "292: loss=0.763, reward_mean=0.570\n",
      "Reward bound 0.0\n",
      "293: loss=0.784, reward_mean=0.590\n",
      "Reward bound 0.0\n",
      "294: loss=0.770, reward_mean=0.520\n",
      "Reward bound 0.0\n",
      "295: loss=0.797, reward_mean=0.550\n",
      "Reward bound 0.0\n",
      "296: loss=0.801, reward_mean=0.480\n",
      "Reward bound 0.0\n",
      "297: loss=0.776, reward_mean=0.530\n",
      "Reward bound 0.0\n",
      "298: loss=0.766, reward_mean=0.550\n",
      "Reward bound 0.0\n",
      "299: loss=0.780, reward_mean=0.570\n",
      "Reward bound 0.0\n",
      "300: loss=0.808, reward_mean=0.610\n",
      "Reward bound 0.0\n",
      "301: loss=0.757, reward_mean=0.580\n",
      "Reward bound 0.0\n",
      "302: loss=0.800, reward_mean=0.620\n",
      "Reward bound 0.0\n",
      "303: loss=0.793, reward_mean=0.610\n",
      "Reward bound 0.0\n",
      "304: loss=0.790, reward_mean=0.570\n",
      "Reward bound 0.0\n",
      "305: loss=0.748, reward_mean=0.650\n",
      "Reward bound 0.0\n",
      "306: loss=0.770, reward_mean=0.580\n",
      "Reward bound 0.0\n",
      "307: loss=0.794, reward_mean=0.580\n",
      "Reward bound 0.0\n",
      "308: loss=0.824, reward_mean=0.550\n",
      "Reward bound 0.0\n",
      "309: loss=0.767, reward_mean=0.580\n",
      "Reward bound 0.0\n",
      "310: loss=0.742, reward_mean=0.640\n",
      "Reward bound 0.0\n",
      "311: loss=0.773, reward_mean=0.600\n",
      "Reward bound 0.0\n",
      "312: loss=0.741, reward_mean=0.510\n",
      "Reward bound 0.0\n",
      "313: loss=0.760, reward_mean=0.610\n",
      "Reward bound 0.0\n",
      "314: loss=0.791, reward_mean=0.640\n",
      "Reward bound 0.0\n",
      "315: loss=0.764, reward_mean=0.590\n",
      "Reward bound 0.0\n",
      "316: loss=0.758, reward_mean=0.590\n",
      "Reward bound 0.0\n",
      "317: loss=0.769, reward_mean=0.560\n",
      "Reward bound 0.0\n",
      "318: loss=0.751, reward_mean=0.640\n",
      "Reward bound 0.06204056683756748\n",
      "319: loss=0.757, reward_mean=0.700\n",
      "Reward bound 0.0\n",
      "320: loss=0.731, reward_mean=0.630\n",
      "Reward bound 0.0\n",
      "321: loss=0.785, reward_mean=0.670\n",
      "Reward bound 0.0\n",
      "322: loss=0.741, reward_mean=0.640\n",
      "Reward bound 0.0\n",
      "323: loss=0.777, reward_mean=0.610\n",
      "Reward bound 0.0\n",
      "324: loss=0.773, reward_mean=0.690\n",
      "Reward bound 0.0\n",
      "325: loss=0.757, reward_mean=0.560\n",
      "Reward bound 0.0\n",
      "326: loss=0.762, reward_mean=0.660\n",
      "Reward bound 0.0\n",
      "327: loss=0.774, reward_mean=0.580\n",
      "Reward bound 0.0\n",
      "328: loss=0.749, reward_mean=0.680\n",
      "Reward bound 0.0\n",
      "329: loss=0.758, reward_mean=0.640\n",
      "Reward bound 0.0\n",
      "330: loss=0.757, reward_mean=0.630\n",
      "Reward bound 0.07659329239205862\n",
      "331: loss=0.756, reward_mean=0.700\n",
      "Reward bound 0.0\n",
      "332: loss=0.726, reward_mean=0.650\n",
      "Reward bound 0.0\n",
      "333: loss=0.748, reward_mean=0.610\n",
      "Reward bound 0.0\n",
      "334: loss=0.761, reward_mean=0.600\n",
      "Reward bound 0.16176866248676577\n",
      "335: loss=0.721, reward_mean=0.710\n",
      "Reward bound 0.0\n",
      "336: loss=0.755, reward_mean=0.660\n",
      "Reward bound 0.09847709021836118\n",
      "337: loss=0.745, reward_mean=0.720\n",
      "Reward bound 0.0\n",
      "338: loss=0.723, reward_mean=0.570\n",
      "Reward bound 0.08510365821339846\n",
      "339: loss=0.749, reward_mean=0.700\n",
      "Reward bound 0.0\n",
      "340: loss=0.759, reward_mean=0.680\n",
      "Reward bound 0.19971439813180958\n",
      "341: loss=0.735, reward_mean=0.760\n",
      "Reward bound 0.22190488681312176\n",
      "342: loss=0.674, reward_mean=0.740\n",
      "Reward bound 0.0\n",
      "343: loss=0.679, reward_mean=0.680\n",
      "Reward bound 0.0\n",
      "344: loss=0.723, reward_mean=0.600\n",
      "Reward bound 0.17974295831862863\n",
      "345: loss=0.722, reward_mean=0.730\n",
      "Reward bound 0.19971439813180958\n",
      "346: loss=0.709, reward_mean=0.760\n",
      "Reward bound 0.10506624470789933\n",
      "347: loss=0.727, reward_mean=0.700\n",
      "Reward bound 0.0\n",
      "348: loss=0.720, reward_mean=0.690\n",
      "Reward bound 0.16677181699666577\n",
      "349: loss=0.735, reward_mean=0.720\n",
      "Reward bound 0.0\n",
      "350: loss=0.697, reward_mean=0.610\n",
      "Reward bound 0.0\n",
      "351: loss=0.727, reward_mean=0.680\n",
      "Reward bound 0.0\n",
      "352: loss=0.672, reward_mean=0.670\n",
      "Reward bound 0.0\n",
      "353: loss=0.705, reward_mean=0.660\n",
      "Reward bound 0.0\n",
      "354: loss=0.742, reward_mean=0.690\n",
      "Reward bound 0.13103261661428028\n",
      "355: loss=0.732, reward_mean=0.710\n",
      "Reward bound 0.0\n",
      "356: loss=0.675, reward_mean=0.650\n",
      "Reward bound 0.08510365821339846\n",
      "357: loss=0.732, reward_mean=0.700\n",
      "Reward bound 0.11792935495285226\n",
      "358: loss=0.673, reward_mean=0.710\n",
      "Reward bound 0.17974295831862863\n",
      "359: loss=0.755, reward_mean=0.720\n",
      "Reward bound 0.19971439813180958\n",
      "360: loss=0.723, reward_mean=0.730\n",
      "Reward bound 0.2541865828329001\n",
      "361: loss=0.683, reward_mean=0.770\n",
      "Reward bound 0.22190488681312176\n",
      "362: loss=0.663, reward_mean=0.800\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "iter_no = 0\n",
    "reward_mean = 0\n",
    "\n",
    "full_batch  = []\n",
    "\n",
    "batch : List[Episode] = []\n",
    "\n",
    "episode_steps = []\n",
    "episode_reward = 0.0\n",
    "\n",
    "state = environment.reset()\n",
    "counter = 0\n",
    "\n",
    "while reward_mean < REWARD_GOAL:\n",
    "    counter += 1\n",
    "    # if counter >= 10:\n",
    "    #     break\n",
    "\n",
    "    action = select_action(state)\n",
    "    next_state, reward, episode_id_done,_ =  environment.step(action)\n",
    "\n",
    "    episode_steps.append( EpisodeStep(observation=state, action=action) )\n",
    "    episode_reward += reward\n",
    "\n",
    "    if episode_id_done :\n",
    "        batch.append( Episode(reward=episode_reward, steps=episode_steps) )\n",
    "        next_state = environment.reset()\n",
    "\n",
    "        episode_steps = []\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        if len(batch) == BATCH_SIZE:\n",
    "            reward_mean = float(np.mean(list(map( lambda s : s.reward , batch))))\n",
    "            elite_candidates = batch\n",
    "\n",
    "            returnG  = list(map( lambda s : s.reward * (GAMMA ** len(s.steps) ) , elite_candidates))\n",
    "            reward_bound = np.percentile(returnG, PERCENTILE)\n",
    "\n",
    "            #z_lambda = lambda z : z.action\n",
    "            #print(\"Elite candidates: \", list(map( lambda s: list(map( lambda z: z.action , s.steps)) ,elite_candidates)) )\n",
    "            #print(\"Return G: \", returnG)\n",
    "            print(\"Reward bound\", reward_bound)\n",
    "\n",
    "            train_obs = []\n",
    "            train_act = []\n",
    "            elite_batch = []\n",
    "            for example, discounted_reward in zip(elite_candidates, returnG):\n",
    "                    if discounted_reward > reward_bound:\n",
    "                            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "                            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "                            elite_batch.append(example)\n",
    "            full_batch=elite_batch\n",
    "            state=train_obs\n",
    "            acts=train_act\n",
    "\n",
    "            if len(full_batch) != 0 : # just in case empty during an iteration\n",
    "                 state_tensor = torch.FloatTensor(state)\n",
    "                 acts_tensor = torch.LongTensor(acts)\n",
    "                 optimizer.zero_grad()\n",
    "                 action_scores_tensor = net(state_tensor)\n",
    "                 loss_tensor = objective(action_scores_tensor, acts_tensor)\n",
    "                 loss_tensor.backward()\n",
    "                 optimizer.step()\n",
    "                 print(\"%d: loss=%.3f, reward_mean=%.3f\" % (iter_no, loss_tensor.item(), reward_mean))\n",
    "                 iter_no += 1\n",
    "            batch = []\n",
    "    state = next_state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "reward =  1.0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "reward =  0.0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "reward =  1.0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "reward =  1.0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "reward =  1.0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "reward =  0.0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "reward =  1.0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHF\u001b[41mH\u001b[0m\n",
      "FFFH\n",
      "HFFG\n",
      "reward =  0.0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "reward =  0.0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "reward =  1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "        test_env = OneHotWrapper(gym.make(\"FrozenLake-v1\", \n",
    "           is_slippery=False))\n",
    "        state= test_env.reset()\n",
    "        test_env.render()\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "                action = select_action(state)\n",
    "                new_state, reward, is_done, _ = test_env.step(action)\n",
    "                test_env.render()\n",
    "                state = new_state\n",
    "        print(\"reward = \", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13027ef6b548d478524874388772865ee1e69188a7bf08a433874e841914c872"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('gpu-pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
