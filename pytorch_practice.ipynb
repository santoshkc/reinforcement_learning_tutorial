{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Squeeze, UnSqueeze, Gather function in pytorch**\n",
    "\n",
    "Use of squeeze, unsqueeze, gather function to compute desired action by indexing  \n",
    "2d-array in the second dimension using action parameter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "Actions:  tensor([1, 3, 2, 0, 1])\n",
      "Unsqueze 1-d:  tensor([[1, 3, 2, 0, 1]])\n",
      "Unsqueze 1-d:  tensor([[1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1]])\n",
      "Unsqueze 1-d:  tensor([[1],\n",
      "        [3],\n",
      "        [2],\n",
      "        [0],\n",
      "        [1]])\n",
      "********************\n",
      "Indices tensor([[ 1,  2,  3,  4],\n",
      "        [ 3,  2, 34, 12],\n",
      "        [17, 23,  2, 34],\n",
      "        [27,  3,  1,  3],\n",
      "        [ 1,  4,  5,  1]])\n",
      "torch.Size([5, 4])\n",
      "Gather:  tensor([[ 2],\n",
      "        [12],\n",
      "        [ 2],\n",
      "        [27],\n",
      "        [ 4]])\n",
      "Original Array:  tensor([[ 1,  2,  3,  4],\n",
      "        [ 3,  2, 34, 12],\n",
      "        [17, 23,  2, 34],\n",
      "        [27,  3,  1,  3],\n",
      "        [ 1,  4,  5,  1]])\n",
      "Max across dim 1:  tensor([ 4, 34, 34, 27,  5]) argmax:  tensor([3, 2, 3, 0, 2])\n",
      "Max across dim 0:  tensor([27, 23, 34, 34]) argmax:  tensor([3, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "two_dim_array = np.array( [[1,2,3,4],[3,2,34,12], [17,23,2,34], [27,3,1,3], [1,4,5,1]] , dtype=np.int64)\n",
    "\n",
    "print(two_dim_array.shape)\n",
    "\n",
    "actions = torch.LongTensor(np.array([1,3,2,0,1]))\n",
    "\n",
    "print(\"Actions: \", actions)\n",
    "# increase dimension\n",
    "print(\"Unsqueze 1-d: \", actions.unsqueeze(0))\n",
    "print(\"Unsqueze 1-d: \", actions.unsqueeze(1))\n",
    "print(\"Unsqueze 1-d: \", actions.unsqueeze(-1))\n",
    "\n",
    "print(\"**\"*10)\n",
    "indices = torch.LongTensor(two_dim_array)\n",
    "print('Indices', indices)\n",
    "print(indices.shape)\n",
    "\n",
    "# gather is differentiable operation\n",
    "# so will keep gradient as well with respect to final loss value\n",
    "\n",
    "# along 2nd dimesion, use 2nd param i.e., one element index array to obtain item \n",
    "print(\"Gather: \", indices.gather(1, actions.unsqueeze(-1)))\n",
    "\n",
    "print(\"Original Array: \", indices)\n",
    "print(\"Max across dim 1: \", indices.max(1)[0], \"argmax: \", indices.max(1)[1])\n",
    "print(\"Max across dim 0: \", indices.max(0)[0], \"argmax: \", indices.max(0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Line fitting use least square error**\n",
    "\n",
    "Gradient computation is manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Epoch: 0, weights: tensor([30.6046]), bias: tensor([7.9204])\n",
      "After Epoch: 20, weights: tensor([30.2899]), bias: tensor([14.2499])\n",
      "After Epoch: 40, weights: tensor([30.1381]), bias: tensor([17.2628])\n",
      "After Epoch: 60, weights: tensor([30.0656]), bias: tensor([18.6968])\n",
      "After Epoch: 80, weights: tensor([30.0313]), bias: tensor([19.3796])\n",
      "After Epoch: 100, weights: tensor([30.0148]), bias: tensor([19.7047])\n",
      "After Epoch: 120, weights: tensor([30.0070]), bias: tensor([19.8594])\n",
      "After Epoch: 140, weights: tensor([30.0034]), bias: tensor([19.9331])\n",
      "After Epoch: 160, weights: tensor([30.0016]), bias: tensor([19.9681])\n",
      "After Epoch: 180, weights: tensor([30.0006]), bias: tensor([19.9848])\n",
      "After Epoch: 200, weights: tensor([30.0003]), bias: tensor([19.9928])\n",
      "After Epoch: 220, weights: tensor([30.0002]), bias: tensor([19.9966])\n",
      "After Epoch: 240, weights: tensor([30.0002]), bias: tensor([19.9983])\n",
      "After Epoch: 260, weights: tensor([30.0001]), bias: tensor([19.9992])\n",
      "After Epoch: 280, weights: tensor([30.0000]), bias: tensor([19.9997])\n",
      "After Epoch: 300, weights: tensor([30.0001]), bias: tensor([19.9997])\n",
      "After Epoch: 320, weights: tensor([30.0001]), bias: tensor([19.9997])\n",
      "After Epoch: 340, weights: tensor([30.0001]), bias: tensor([19.9997])\n",
      "After Epoch: 360, weights: tensor([30.0001]), bias: tensor([19.9997])\n",
      "After Epoch: 380, weights: tensor([30.0001]), bias: tensor([19.9997])\n",
      "After Epoch: 400, weights: tensor([30.0001]), bias: tensor([19.9997])\n",
      "After Epoch: 420, weights: tensor([30.0001]), bias: tensor([19.9997])\n",
      "After Epoch: 440, weights: tensor([30.0001]), bias: tensor([19.9997])\n",
      "After Epoch: 460, weights: tensor([30.0001]), bias: tensor([19.9997])\n",
      "After Epoch: 480, weights: tensor([30.0001]), bias: tensor([19.9997])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def least_squares_fitting():\n",
    "    weights = torch.rand((1,),requires_grad=False)\n",
    "    bias = torch.rand((1,),  requires_grad=False)\n",
    "\n",
    "    data_points = [ (torch.Tensor([i]), torch.Tensor([20 + 30*i]) ) for i in range(20)]\n",
    "    inputs, actual_outputs = zip(*data_points)\n",
    "\n",
    "    inputs = torch.Tensor(inputs)\n",
    "    actual = torch.Tensor(actual_outputs)\n",
    "\n",
    "    alpha = 0.01\n",
    "    optimizer = torch.optim.Adam([weights,bias], lr=alpha)\n",
    "\n",
    "    for epoch in range(500):\n",
    "        for iteration in range(20):\n",
    "            input = inputs[iteration]\n",
    "\n",
    "            predict = weights * input + bias\n",
    "            diff = (actual[iteration] - predict)\n",
    "            loss  = -1/2*(diff)**2\n",
    "\n",
    "            #grad_loss = grad(loss) = -diff\n",
    "            grad_weight = (diff)*input\n",
    "            grad_bias = (diff)*1\n",
    "\n",
    "            #print(\"Gradients: \", grad_weight, grad_bias)\n",
    "\n",
    "            weights +=  alpha*grad_weight\n",
    "            bias +=  alpha*grad_bias\n",
    "\n",
    "        if epoch%20 == 0:\n",
    "            print(f\"After Epoch: {epoch}, weights: {weights}, bias: {bias}\")\n",
    "\n",
    "least_squares_fitting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Propagation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Error: 120992.140625\n",
      "Epoch: 500, Error: 1.1008524092304128e-09\n",
      "Weights: tensor([30.0000], requires_grad=True), bias: tensor([20.0000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def least_squares_gradient_propagation():\n",
    "    weights = torch.rand((1,),requires_grad=True)\n",
    "    bias = torch.rand((1,),  requires_grad=True)\n",
    "\n",
    "    data_points = [ (torch.Tensor([i]), torch.Tensor([20 + 30*i]) ) for i in range(20)]\n",
    "    inputs, actual_outputs = zip(*data_points)\n",
    "\n",
    "    inputs = torch.Tensor(inputs)\n",
    "    actual = torch.Tensor(actual_outputs)\n",
    "\n",
    "    # alpha with 0.01 seems to work in case of Adam\n",
    "    # alpha = 0.2\n",
    "    # optimizer = torch.optim.Adam([weights,bias], lr=alpha)\n",
    "    # total_epochs = 4000\n",
    "    # high value of alpha did not worked in case of SGD, providing momentum helped for faster convergence\n",
    "    alpha = 0.01\n",
    "    optimizer = torch.optim.SGD( [weights, bias], lr=alpha,momentum=0.95)\n",
    "    total_epochs = 1000\n",
    "\n",
    "    for epoch in range(total_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        total_error = 0.0\n",
    "        predicted = weights*inputs + bias\n",
    "        loss = torch.mean((actual - predicted)**2)\n",
    "        total_error += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch%500 == 0:\n",
    "            print(f\"Epoch: {epoch}, Error: {total_error}\")\n",
    "\n",
    "    print(f\"Weights: {weights}, bias: {bias}\")\n",
    "\n",
    "least_squares_gradient_propagation()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13027ef6b548d478524874388772865ee1e69188a7bf08a433874e841914c872"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('gpu-pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
